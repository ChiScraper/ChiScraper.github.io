<!DOCTYPE html>
<html lang="en">

<head>

<title>ChiScraper</title>
<meta name="description" content="SHORT DESCRIPTION OF YOUR PAGE" />
<meta content="text/html;charset=utf-8" http-equiv="Content-Type" />
<meta content="utf-8" http-equiv="encoding" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link href="favicon.ico" rel="icon" type="image/x-icon" />
<link href="./style.css" rel="stylesheet" />

<script>
    // Template generated with petrapixel's layout generator.
    // Please do not remove this.
    console.log("%c Template generated with petrapixel's layout generator.", "font-size: 14pt;");
    console.log("%c https://petrapixel.neocities.org/coding/layout-generator", "font-size: 14pt;");
</script>
</head>

<body>
<!-- The next line is a skip-to-content link for keyboard users. Do not remove it! -->
<a href="#content" id="skip-to-content-link">Skip to content</a>

<div class="layout">
	
<header>
    
<div class="header-content">
	<div class="header-title" style="display: inline-block; vertical-align: top;">Chi-Scraper</div>
	<img src="./Assets/ChiCurateRound.svg" alt="ChiCurate Icon" class="header-icon" style="display: inline-block; margin-left: auto; float: right;">
</div>

</header>

<aside class="left-sidebar">
    
<nav>
    <div class="sidebar-title">Navigation</div>
    <ul>
      <li><a href="./index">Home</a></li>
      <li><a href="./install.html">Install</a></li>
      <li><a href="./Use.html">Run</a></li>
      <li><a href="./Licence">Licence</a></li>
      <li>
      <details>
        <summary><a href="./HelpDocs">Help</a></summary>
                <ul>
                  <li><a href="./aiRanking">AI Ranking</a></li>
                  <li><a href="./localAIEval">Local AI Evaluation</a></li>
                  <li><a href="./searchConfig">Filter Configuration</a></li>
                </ul>
      </details>
      </li>
      <li><a href="./FAQs">FAQs </a> </li>
      <li><a href="./Contributing">Contributing</a></li>

  </ul>
</nav>

<div class="sidebar-section">
  <div class="sidebar-title">About The Program</div>
  <blockquote>
    <p>This tool helps you turn the firehose of content from the ArXiv into a pleasent trickle you can take a sip from every day.</p>
  </blockquote>
</div>


<div class="sidebar-section">
  <div class="sidebar-title">Project</div>
  <ul>
    <li><a href="https://github.com/ChiScraper/ChiScraper">GitHub</a></li>
  </ul>
</div>
       
</aside>

<main id="content">
    <h1>Quickstart</h1>
<ol>
<li>Install <a href="https://ollama.com/">Ollama</a></li>
<li>Pull Llama3.1 8b from the <a href="https://ollama.com/library">Ollama Model Repository</a></li>
</ol>
<pre><code class="language-bash">ollama pull llama3.18b
</code></pre>
<ol start="3">
<li>Edit <code>host</code> and <code>ai_model</code> in the config</li>
</ol>
<pre><code class="language-yaml">host: local
ai_model: llama3.1:8b
</code></pre>
<p><strong>Detailed instructions and configuration below</strong></p>
<h1>Intructions for AI Evalutation</h1>
<p>To use the AI-LLM capabilities, you will need to install some an external program called &quot;Ollama&quot;. This is a piece of middleware that lets you host LLMs locally.</p>
<h2>Installing Ollama</h2>
<h3>Linux</h3>
<pre><code class="language-bash">curl -fsSL https://ollama.com/install.sh | sh
</code></pre>
<h3>Mac / Windows</h3>
<p>Install from the relevant links on the Ollama website</p>
<h2>Verify Ollama is running.</h2>
<p>Navigating to <a href="http://localhost:11434">http://localhost:11434</a> in your web browser will tell you if it is running correctly.</p>
<h2>Pull a model</h2>
<p>Ollama has a whole bunch of models to pick from. My reccomendation is Llama3 8b. This can be downloaded.</p>
<pre><code>ollama pull llama3.1:8b
</code></pre>
<p>If you choose another model, ensure you update the config.</p>
<h3>Picking other models.</h3>
<p>I reccomend Llama3 8b because it balences system requirements with performance. The model takes about 45s per article on my laptop. For more intelligent reccomendation, try a larger model. e.g. <code>mistral-nemo:12b</code>. If you want faster performance, and can tolerate dumber reccomendations, consider phi3 mini.</p>
<p>See the avaliable models at the <a href="https://ollama.com/library">Ollama Model Repository</a></p>
<h1>Configuring Chi-Scraper</h1>
<p>Within <code>config.yaml</code> set the host to <code>local</code> and the <code>ai_model</code> to whichever model you have downloaded and plan on using.</p>
<pre><code class="language-yaml">host: local
ai_model: llama3.1:8b
</code></pre>

</main>


<footer>

<div>All rights reserved to me. Last updated 2024-10-17.</div>

</footer>

</div>
</body>
</html>
